from metagrad.tensor import Tensor
import numpy as np
import torch


def test_simple_std():
    a = np.array([1, 2, 3])
    x = Tensor(a, requires_grad=True)
    y = x.std()
    y.sum().backward()

    assert y.item() == 0.816496580927726

    assert x.grad.tolist() == [-0.408248290463863, 0.0, 0.408248290463863]


def test_matrix_std():
    a = np.array([[0.2035, 1.2959, 1.8101, -0.4644],
                  [1.5027, -0.3270, 0.5905, 0.6538],
                  [-1.5745, 1.3330, -0.5596, -0.6548],
                  [0.1264, -0.5080, 1.6420, 0.1992]])
    x = Tensor(a, requires_grad=True)
    y = x.std()
    y.sum().backward()

    assert y.item() == 0.9561251101451107
    assert x.grad.tolist() == [[-0.008223296215708331,
                                0.06318472275122157,
                                0.09679695577282113,
                                -0.051882593055705105],
                               [0.07670282813602668,
                                -0.042901027872570566,
                                0.017074125369976285,
                                0.021211920683603766],
                               [-0.12444762587810429,
                                0.06560987608669676,
                                -0.058105628029754657,
                                -0.0643286630038042],
                               [-0.013263170128515264,
                                -0.0547326384849967,
                                0.08580859254658446,
                                -0.008504378677771493]]


def test_matrix_with_axis():
    a = np.array([[0.2035, 1.2959, 1.8101, -0.4644],
                  [1.5027, -0.3270, 0.5905, 0.6538],
                  [-1.5745, 1.3330, -0.5596, -0.6548],
                  [0.1264, -0.5080, 1.6420, 0.1992]])
    x = Tensor(a, requires_grad=True)
    y = x.std(axis=1)
    y.sum().backward()

    assert y.tolist() == [0.8929235588083675, 0.6475108068596229, 1.0568754949732728, 0.7869684809444403]
    assert x.grad.tolist() == [[-0.14216642482746242,
                                0.26675230462593025,
                                0.5142694220701418,
                                -0.2634476539024656],
                               [0.22158251739269252,
                                -0.3598395540763742,
                                0.2257775406232069,
                                0.0917762296062008],
                               [-0.6399693953227176,
                                0.2810763898793996,
                                -0.046274372177809615,
                                -0.3239329225664346],
                               [-0.16375281910484385,
                                -0.4297225576040821,
                                0.4745059871150501,
                                -0.05263870282363265]]


def test_matrix_with_keepdims():
    a = np.array([[0.2035, 1.2959, 1.8101, -0.4644],
                  [1.5027, -0.3270, 0.5905, 0.6538],
                  [-1.5745, 1.3330, -0.5596, -0.6548],
                  [0.1264, -0.5080, 1.6420, 0.1992]])
    x = Tensor(a, requires_grad=True)
    y = x.std(axis=1, keepdims=True)
    y.sum().backward()

    assert y.tolist() == [[0.8929235588083675], [0.6475108068596229], [1.0568754949732728], [0.7869684809444403]]

    assert x.grad.tolist() == [[-0.14216642482746242,
                                0.16368282431146713,
                                0.3076481153287114,
                                -0.329164514812716],
                               [0.3465965318609025,
                                -0.3598395540763742,
                                -0.005598362161059454,
                                0.018841384376531208],
                               [-0.2863452236705074,
                                0.40141317687636296,
                                -0.046274372177809615,
                                -0.06879358102804593],
                               [-0.07576542319515019,
                                -0.2772982721469459,
                                0.4057023981657287,
                                -0.05263870282363265]]
